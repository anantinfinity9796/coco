{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm, trange\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "from torchvision.transforms import Compose,  Resize, CenterCrop, Normalize\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('../runs/resnet_exp_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_properties(device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Reading the image and the captions dataframe and the labels dataframe which has the super categories.\n",
    "image_df = pd.read_csv('F://coco/captions/final_captions.csv',)\n",
    "labels_df = pd.read_csv('F://coco/captions/labels_information_full.csv')\n",
    "\n",
    "# Also we need to read the multilabel dataframe which has the multilabel infomration about the images.\n",
    "multilabel_df = pd.read_csv('F://coco/captions/multilabel_labels.csv').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_df = image_df[['image_id','file_name']].merge(multilabel_df, on='image_id',validate='1:1')\n",
    "image_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_channel_list = ['COCO_train2014_000000061048.jpg',\n",
    "'COCO_train2014_000000434765.jpg',\n",
    "'COCO_train2014_000000053756.jpg',\n",
    "'COCO_train2014_000000470933.jpg',\n",
    "'COCO_train2014_000000571415.jpg',\n",
    "'COCO_train2014_000000492325.jpg',\n",
    "'COCO_train2014_000000155083.jpg',\n",
    "'COCO_train2014_000000421613.jpg',\n",
    "'COCO_train2014_000000431115.jpg',\n",
    "'COCO_train2014_000000173610.jpg',\n",
    "'COCO_train2014_000000336668.jpg',\n",
    "'COCO_train2014_000000316867.jpg',\n",
    "'COCO_train2014_000000269858.jpg',\n",
    "'COCO_train2014_000000250239.jpg',\n",
    "'COCO_train2014_000000123539.jpg',\n",
    "'COCO_train2014_000000140092.jpg',\n",
    "'COCO_train2014_000000343009.jpg',\n",
    "'COCO_train2014_000000003293.jpg',\n",
    "'COCO_train2014_000000578250.jpg',\n",
    "'COCO_train2014_000000518025.jpg',\n",
    "'COCO_train2014_000000008794.jpg',\n",
    "'COCO_train2014_000000400107.jpg',\n",
    "'COCO_train2014_000000394547.jpg',\n",
    "'COCO_train2014_000000389984.jpg',\n",
    "'COCO_train2014_000000225717.jpg',\n",
    "'COCO_train2014_000000032405.jpg',\n",
    "'COCO_train2014_000000084582.jpg',\n",
    "'COCO_train2014_000000549879.jpg',\n",
    "'COCO_train2014_000000358281.jpg',\n",
    "'COCO_train2014_000000457741.jpg',\n",
    "'COCO_train2014_000000204792.jpg',\n",
    "'COCO_train2014_000000124694.jpg',\n",
    "'COCO_train2014_000000390663.jpg',\n",
    "'COCO_train2014_000000179405.jpg',\n",
    "'COCO_train2014_000000443909.jpg',\n",
    "'COCO_train2014_000000268036.jpg',\n",
    "'COCO_train2014_000000217341.jpg',\n",
    "'COCO_train2014_000000363331.jpg',\n",
    "'COCO_train2014_000000134071.jpg',\n",
    "'COCO_train2014_000000505962.jpg',\n",
    "'COCO_train2014_000000347111.jpg',\n",
    "'COCO_train2014_000000484742.jpg',\n",
    "'COCO_train2014_000000064270.jpg',\n",
    "'COCO_train2014_000000012345.jpg',\n",
    "'COCO_train2014_000000226585.jpg',\n",
    "'COCO_train2014_000000575029.jpg',\n",
    "'COCO_train2014_000000564314.jpg',\n",
    "'COCO_train2014_000000011801.jpg',\n",
    "'COCO_train2014_000000033127.jpg',\n",
    "'COCO_train2014_000000312288.jpg',\n",
    "'COCO_train2014_000000126531.jpg',\n",
    "'COCO_train2014_000000140627.jpg',\n",
    "'COCO_train2014_000000131366.jpg',\n",
    "'COCO_train2014_000000406011.jpg',\n",
    "'COCO_train2014_000000369966.jpg',\n",
    "'COCO_train2014_000000081003.jpg',\n",
    "'COCO_train2014_000000010125.jpg',\n",
    "'COCO_train2014_000000006432.jpg',\n",
    "'COCO_train2014_000000384693.jpg',\n",
    "'COCO_train2014_000000470442.jpg',\n",
    "'COCO_train2014_000000280731.jpg',\n",
    "'COCO_train2014_000000113929.jpg',\n",
    "'COCO_train2014_000000416869.jpg',\n",
    "'COCO_train2014_000000066642.jpg',\n",
    "'COCO_train2014_000000233263.jpg',\n",
    "'COCO_train2014_000000025404.jpg',\n",
    "'COCO_train2014_000000156878.jpg',\n",
    "'COCO_train2014_000000166522.jpg',\n",
    "'COCO_train2014_000000060060.jpg',\n",
    "'COCO_train2014_000000445845.jpg',\n",
    "'COCO_train2014_000000205486.jpg',\n",
    "'COCO_train2014_000000577207.jpg',\n",
    "'COCO_train2014_000000005294.jpg',\n",
    "'COCO_train2014_000000186888.jpg',\n",
    "'COCO_train2014_000000503640.jpg',\n",
    "'COCO_train2014_000000000086.jpg',\n",
    "'COCO_train2014_000000087509.jpg',\n",
    "'COCO_train2014_000000571503.jpg',\n",
    "'COCO_train2014_000000000821.jpg',\n",
    "'COCO_train2014_000000579138.jpg',\n",
    "'COCO_train2014_000000134918.jpg',\n",
    "'COCO_train2014_000000259284.jpg',\n",
    "'COCO_train2014_000000257178.jpg',\n",
    "'COCO_train2014_000000221691.jpg',\n",
    "'COCO_train2014_000000077709.jpg',\n",
    "'COCO_train2014_000000263002.jpg',\n",
    "'COCO_train2014_000000341892.jpg',\n",
    "'COCO_train2014_000000349069.jpg',\n",
    "'COCO_train2014_000000563376.jpg',\n",
    "'COCO_train2014_000000220770.jpg',\n",
    "'COCO_train2014_000000208206.jpg',\n",
    "'COCO_train2014_000000027412.jpg',\n",
    "'COCO_train2014_000000434837.jpg',\n",
    "'COCO_train2014_000000080906.jpg',\n",
    "'COCO_train2014_000000150354.jpg',\n",
    "'COCO_train2014_000000107450.jpg',\n",
    "'COCO_train2014_000000577265.jpg',\n",
    "'COCO_train2014_000000416372.jpg',\n",
    "'COCO_train2014_000000377837.jpg',\n",
    "'COCO_train2014_000000579239.jpg',\n",
    "'COCO_train2014_000000540378.jpg',\n",
    "'COCO_train2014_000000525513.jpg',\n",
    "'COCO_train2014_000000353952.jpg',\n",
    "'COCO_train2014_000000006379.jpg',\n",
    "'COCO_train2014_000000381270.jpg',\n",
    "'COCO_train2014_000000520479.jpg',\n",
    "'COCO_train2014_000000563447.jpg',\n",
    "'COCO_train2014_000000085407.jpg',\n",
    "'COCO_train2014_000000210175.jpg',\n",
    "'COCO_train2014_000000397575.jpg',\n",
    "'COCO_train2014_000000058517.jpg',\n",
    "'COCO_train2014_000000384907.jpg',\n",
    "'COCO_train2014_000000509358.jpg',\n",
    "'COCO_train2014_000000264165.jpg',\n",
    "'COCO_train2014_000000072098.jpg',\n",
    "'COCO_train2014_000000155954.jpg',\n",
    "'COCO_train2014_000000270925.jpg',\n",
    "'COCO_train2014_000000104124.jpg',\n",
    "'COCO_train2014_000000095753.jpg',\n",
    "'COCO_train2014_000000210847.jpg',\n",
    "'COCO_train2014_000000507794.jpg',\n",
    "'COCO_train2014_000000561842.jpg',\n",
    "'COCO_train2014_000000249835.jpg',\n",
    "'COCO_train2014_000000361516.jpg',\n",
    "'COCO_train2014_000000451074.jpg',\n",
    "'COCO_train2014_000000480482.jpg',\n",
    "'COCO_train2014_000000220898.jpg',\n",
    "'COCO_train2014_000000260962.jpg',\n",
    "'COCO_train2014_000000576700.jpg',\n",
    "'COCO_train2014_000000296884.jpg',\n",
    "'COCO_train2014_000000342921.jpg',\n",
    "'COCO_train2014_000000384910.jpg',\n",
    "'COCO_train2014_000000040428.jpg',\n",
    "'COCO_train2014_000000145288.jpg',\n",
    "'COCO_train2014_000000321897.jpg',\n",
    "'COCO_train2014_000000449901.jpg',\n",
    "'COCO_train2014_000000107962.jpg',\n",
    "'COCO_train2014_000000001350.jpg',\n",
    "'COCO_train2014_000000249711.jpg',\n",
    "'COCO_train2014_000000140623.jpg',\n",
    "'COCO_train2014_000000211867.jpg',\n",
    "'COCO_train2014_000000496444.jpg',\n",
    "'COCO_train2014_000000287422.jpg',\n",
    "'COCO_train2014_000000118895.jpg',\n",
    "'COCO_train2014_000000075052.jpg',\n",
    "'COCO_train2014_000000436984.jpg',\n",
    "'COCO_train2014_000000555583.jpg',\n",
    "'COCO_train2014_000000029275.jpg',\n",
    "'COCO_train2014_000000176397.jpg',\n",
    "'COCO_train2014_000000034861.jpg',\n",
    "'COCO_train2014_000000517899.jpg',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for file_name in single_channel_list:\n",
    "    index = image_label_df[image_label_df['file_name']==file_name].index.values[0]\n",
    "    indexes.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_df = image_label_df.drop(indexes, axis = 0)\n",
    "image_label_df = image_label_df.reset_index(drop=True)\n",
    "image_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_label_df.to_parquet('F://coco/captions/image_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_transforms = ResNet50_Weights.DEFAULT.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_transforms = Compose([\n",
    "    Resize(size = (256,256)),\n",
    "    CenterCrop(size=(224,224)),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "manual_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self,image_label_df, val_stride = 10, is_val_set_bool = False, test_data_set =False, test_stride = 5, transforms = None):\n",
    "        \n",
    "        self.transfrom = transforms\n",
    "        self.is_val_set_bool = is_val_set_bool\n",
    "        self.val_stride = val_stride\n",
    "        self.image_label_df = image_label_df.copy()\n",
    "        self.test_data_set = test_data_set\n",
    "        self.test_stride = test_stride\n",
    "\n",
    "        if self.test_data_set: #If we need only a small subset of the data to work with\n",
    "            self.image_label_df = self.image_label_df[::test_stride].reset_index(drop = True)\n",
    "\n",
    "        elif self.is_val_set_bool: # If we need only the validation data then return the validation data which is a subset of total data\n",
    "            assert self.val_stride > 0\n",
    "            self.image_label_df = self.image_label_df[::val_stride]\n",
    "\n",
    "        elif self.val_stride > 0:  # Else if val_stride is greater than zero then return the remaining dataframe after removing 10% of data\n",
    "            self.image_label_df = self.image_label_df.drop(index = list(range(0,len(self.image_label_df),self.val_stride)))\n",
    "            \n",
    "        else: # else train on the full dataset\n",
    "            self.image_label_df = self.image_label_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" This method calculates the length of your data.\"\"\"\n",
    "        return len(self.image_label_df)\n",
    "\n",
    "    # Now we will introduce function which gives us an image and its corresponding labels by id\n",
    "    def get_image_by_id(self, img_id = None):\n",
    "        \"\"\" This function returns an image by its id and the corresponding multiple labesl in a present/no_present binary format\"\"\"\n",
    "        \n",
    "\n",
    "        folder_path = 'F://coco/train2014/train2014/'\n",
    "\n",
    "        if img_id == None:\n",
    "            raise ValueError('Must provide IMAGE ID')\n",
    "\n",
    "        else:\n",
    "            row = self.image_label_df[self.image_label_df['image_id']==img_id]\n",
    "            file_name = row['file_name'].values[0]\n",
    "\n",
    "            \n",
    "            label_array = torch.zeros(len(self.image_label_df.columns[2:]),2)  # create an array of zeros which is a (num_classes, 2) array\n",
    "\n",
    "            # Lets get the multilabel array for a particular image_id from image_label_df\n",
    "            multilabel_array = row.values[0][2:]\n",
    "\n",
    "            # Now lets populate the label_array with values from the multilabel array\n",
    "            label_array[range(len(multilabel_array)), multilabel_array] = 1\n",
    "\n",
    "            # Get the image-data from file_name\n",
    "            image_array = torchvision.io.read_image(folder_path + file_name)\n",
    "            image_array = image_array/255.0\n",
    "            image_array = self.transfrom(image_array).to(torch.float32)\n",
    "        \n",
    "            return image_array, label_array\n",
    "\n",
    "\n",
    "    # Now we would write the code for returning the image from index and its corresponding labels. Which would be used by the train/val dataloaders\n",
    "    def __getitem__(self, ndx):\n",
    "\n",
    "        \"\"\" This function takes in an index and returns the image and the labels of the image at that index\"\"\"\n",
    "        folder_path = 'F://coco/train2014/train2014/'\n",
    "        \n",
    "        row = self.image_label_df.iloc[ndx]\n",
    "\n",
    "        # Now get the image_id and the file_name of the image\n",
    "        image_id = row['image_id']\n",
    "        file_name = row['file_name']\n",
    "\n",
    "        # print(image_id, file_name)\n",
    "        # Now get the multilabel in the form of a numpy array\n",
    "        # This array would be of the shape (num_classes, 2). Which means that if a class is present or not present and we will keep 1 at that\n",
    "        # Input probabilities for each class wo\n",
    "        label_array = torch.zeros(len(self.image_label_df.columns[2:]),2)  # create an array of zeros which is a (num_classes, 2) array\n",
    "\n",
    "\n",
    "        # Now we will use the multiple_labels array to populate the categories which are present in the picture.\n",
    "        # For each category we will populate 1 at the 0th position if it is not present or 1 at the 1th position if it is present\n",
    "        # This way we can check for the prescence or absence of multiple categories which makes it a multilabel classification problem\n",
    "\n",
    "        \n",
    "\n",
    "        # Lets get the multilabel array for a particular image_id\n",
    "        multilabel_array = row.values[2:].astype(np.float32)\n",
    "\n",
    "        # Now lets populate the label_array with values from the multilabel array\n",
    "        label_array[range(len(multilabel_array)), multilabel_array] = 1\n",
    "\n",
    "        # Now get the image_data from storage\n",
    "        image_array = torchvision.io.read_image(folder_path + file_name)\n",
    "        image_array = image_array/255.0\n",
    "        image_array = self.transfrom(image_array).to(torch.float32)\n",
    "        return (image_array, label_array, image_id)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coco = CocoDataset(image_label_df=image_label_df, val_stride=10, transforms=manual_transforms)\n",
    "val_coco = CocoDataset(image_label_df=image_label_df,is_val_set_bool=True, val_stride=10, transforms=manual_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_coco, batch_size=128, pin_memory=True, drop_last=True, num_workers=1)\n",
    "val_dataloader = DataLoader(dataset=val_coco, batch_size=128,pin_memory=True, drop_last = True, num_workers=1)\n",
    "\n",
    "# test_coco = CocoDataset(image_label_df=image_label_df, test_data_set=True, test_stride=50, transforms=auto_transforms)\n",
    "\n",
    "# test_coco_dataloader = DataLoader(dataset=test_coco, batch_size=16, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train_data, exp_train_label = [],[]\n",
    "\n",
    "for i in range(128):\n",
    "    img, labels,_ = train_coco[i]\n",
    "    exp_train_data.append(img)\n",
    "    exp_train_label.append(labels)\n",
    "# exp_train_data = torch.tensor(exp_train_data)\n",
    "# exp_train_label = torch.tensor(exp_train_label)\n",
    "# print(exp_train_data.shape)\n",
    "# print(exp_train_label.shape)\n",
    "# print(len(exp_train_data), len(exp_train_label))\n",
    "exp_train_data = torch.stack(exp_train_data)\n",
    "exp_train_label = torch.stack(exp_train_label)\n",
    "exp_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exp_train_data[15].permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, val_dataloader, device, loss_fn):\n",
    "    # print(\"Validation is progressing\")\n",
    "    val_loss_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_img, val_label, _ in tqdm(val_dataloader, nrows = 1):\n",
    "            # transfer the images and labels to the GPU\n",
    "            val_img = val_img.to(device)\n",
    "            val_label = val_label.to(device)\n",
    "\n",
    "            # Now run it through the model and get the val_logits\n",
    "            val_logits = model(val_img)\n",
    "\n",
    "            val_loss = loss_fn(val_logits, val_label)\n",
    "            out_val_loss = val_loss.clone().to(torch.device('cpu'))\n",
    "            val_loss_list.append(out_val_loss.item())\n",
    "            # val_loss += out_val_loss.item()\n",
    "            # val_i +=1\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will begin to write the training loop that will ingest the data and output the result.\n",
    "def training_loop(epochs, conv_model,train_dataloader,device,optimizer= None, val_dataloader=None, loss_fn = None, schedular = None, writer = None):\n",
    "    \n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "        print(\"Training is progressing\")\n",
    "        train_losses = []\n",
    "        # print(\"Training is Progressing\")\n",
    "        for img, label,_ in tqdm(train_dataloader, nrows=1):\n",
    "            batch_losses = []\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Now put the training data into the model to get the logits\n",
    "            pred = conv_model(img)\n",
    "\n",
    "            # Now we will zero out the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            out_loss = loss.clone().to(device=torch.device('cpu'))\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            # Next we will update the parameters\n",
    "            optimizer.step()\n",
    "            # print(out_loss.item())\n",
    "            train_losses.append(out_loss.item())\n",
    "        print(\"Validation is progressing\")\n",
    "        val_loss_list = validation_loop(model = conv_model, val_dataloader=val_dataloader, device = device, loss_fn= loss_fn)\n",
    "        print(f\"Epoch: {epoch} | Train_loss: {torch.tensor(train_losses).mean().item()}) | Val_loss : {torch.tensor(val_loss_list).mean().item()}\")\n",
    "\n",
    "        # step the learning rate schedular\n",
    "        schedular.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Writing the training and the validation losses to tensorboard.\n",
    "        writer.add_scalar(tag = 'Training Loss Per Epoch',scalar_value=torch.tensor(train_losses).mean().item(), global_step = epoch)\n",
    "        writer.add_scalar(tag = 'Validation Loss Per Epoch',scalar_value=torch.tensor(val_loss_list).mean().item(), global_step = epoch)\n",
    "        # break\n",
    "            \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confusion_mat = torch.zeros(12,12)\n",
    "# def make_confusion_mat(logits_tensor, labels_tensor):\n",
    "#     logits_tensor_argmax = torch.argmax(logits_tensor, dim = 2)\n",
    "#     labels_tensor_argmax = torch.argmax(labels_tensor, dim = 2)\n",
    "#     batch_conf_mat = mlcm.cm(labels_tensor_argmax.numpy(), logits_tensor_argmax.numpy(), print_note=False)\n",
    "    \n",
    "#     return batch_conf_mat\n",
    "\n",
    "# # px.imshow(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conf_mat = torch.zeros((13,13))\n",
    "# seq_convnet.eval()\n",
    "# seq_convnet = seq_convnet.to(device=device)\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     for data, label, _ in val_dataloader:\n",
    "#         data = data.to(device = device)\n",
    "        \n",
    "#         # label = label.to(device = device)\n",
    "#         logits = seq_convnet(data)\n",
    "#         label = label.to(device= torch.device('cpu'))\n",
    "#         logits = logits.to(device= torch.device('cpu'))\n",
    "#         batch_conf_mat = make_confusion_mat(logits_tensor= logits, labels_tensor=label)\n",
    "#         conf_mat += batch_conf_mat[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = px.imshow(conf_mat, color_continuous_scale='tempo')\n",
    "# img.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         tickmode = 'array',\n",
    "#         tickvals = [0,1, 2, 3, 4, 5,6, 7,8, 9,10, 11],\n",
    "#         ticktext =['food', 'animal', 'furniture', 'electronic', 'kitchen', 'vehicle',\n",
    "#        'person', 'outdoor', 'accessory', 'sports', 'appliance', 'indoor']\n",
    "#     ),\n",
    "#     yaxis = dict\n",
    "#     (\n",
    "#         tickmode = 'array',\n",
    "#         tickvals = [0,1, 2, 3, 4, 5,6, 7,8, 9,10, 11],\n",
    "#         ticktext = ['food', 'animal', 'furniture', 'electronic', 'kitchen', 'vehicle',\n",
    "#        'person', 'outdoor', 'accessory', 'sports', 'appliance', 'indoor']\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we would implement the resblock which has two 3x3 convolution and see how much we can get out of that training on the coco image dataset.\n",
    "class ResBlock64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 1, padding = 'same')\n",
    "        self.b1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same')\n",
    "        self.b2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1, padding = 'same')\n",
    "        self.b3 =nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Now lets define the computation of the resblock\n",
    "        out = self.relu1(self.b1(self.conv1(x)))\n",
    "        out = self.relu2(self.b2(self.conv2(out)))\n",
    "        out = self.b3(self.conv3(out))\n",
    "        out_final = self.relu3(out + x)\n",
    "        \n",
    "        return out_final\n",
    "\n",
    "class ResBlock128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First 1x1 conv layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=128, out_channels=32, kernel_size=1, padding='same')\n",
    "        self.b1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second 3x3 conv layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding = 'same')\n",
    "        self.b2 =nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Third 1x1 conv layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=1, padding = 'same')\n",
    "        self.b3 =nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Now lets define the computation of the resblock\n",
    "        out = self.relu1(self.b1(self.conv1(x)))\n",
    "        out = self.relu2(self.b2(self.conv2(out)))\n",
    "        out = self.b3(self.conv3(out))\n",
    "        out_final = self.relu3(out + x)\n",
    "        return out_final\n",
    "\n",
    "class downsampleBlock(nn.Module):\n",
    "    \"\"\" This class defines a downsampling block which is used to half the image size and double the channels of an image.\"\"\"\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super().__init__()\n",
    "        # Define the first 1x1 conv layer. This layer would increase the channels of the input\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_chan, out_channels=out_chan, kernel_size=1, stride = 2)\n",
    "        self.b1 = nn.BatchNorm2d(out_chan)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Then define the 3x3 conv layer. This layer would decrease the size of the image while making the channels constant.\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_chan, out_channels=out_chan, kernel_size=3, padding='same')\n",
    "        self.b2 = nn.BatchNorm2d(out_chan)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Now define the third 1x1 layer. This layer would increase the channels of the input so that it \n",
    "        self.conv3 = nn.Conv2d(in_channels = in_chan, out_channels=out_chan, kernel_size = 1, stride = 2)\n",
    "        self.b3 = nn.BatchNorm2d(out_chan)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.relu1(self.b1(self.conv1(X)))\n",
    "        out = self.b2(self.conv2(out))\n",
    "\n",
    "        out_short = self.b3(self.conv3(X))\n",
    "\n",
    "        out_final = self.relu2(out + out_short)\n",
    "\n",
    "        return out_final\n",
    "\n",
    "class ResBlock256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First 1x1 conv layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=1, padding='same')\n",
    "        self.b1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second 3x3 conv layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding = 'same')\n",
    "        self.b2 =nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Third 1x1 conv layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1, padding = 'same')\n",
    "        self.b3 =nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Now lets define the computation of the resblock\n",
    "        out = self.relu1(self.b1(self.conv1(x)))\n",
    "        out = self.relu2(self.b2(self.conv2(out)))\n",
    "        out = self.b3(self.conv3(out))\n",
    "        out_final = self.relu3(out + x)\n",
    "        return out_final\n",
    "\n",
    "class ResBlock512(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First 1x1 conv layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding='same')\n",
    "        self.b1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second 3x3 conv layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding = 'same')\n",
    "        self.b2 =nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Third 1x1 conv layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=1, padding = 'same')\n",
    "        self.b3 =nn.BatchNorm2d(512)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Now lets define the computation of the resblock\n",
    "        out = self.relu1(self.b1(self.conv1(x)))\n",
    "        out = self.relu2(self.b2(self.conv2(out)))\n",
    "        out = self.b3(self.conv3(out))\n",
    "        out_final = self.relu3(out + x)\n",
    "        return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,batch_size = None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        # The first layer will input an  image of 224x224\n",
    "        self.input_conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7,7), padding = 'same') # in = (3,224,224) | out = (64,224,224)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)  # in = (64,224,224) | out = (64,112,112)\n",
    "\n",
    "        self.resblocks64 = nn.Sequential(\n",
    "            *(3*[ResBlock64()]+[downsampleBlock(64,128)]))                      # in = (64,128,128) | out = (64,128,128)\n",
    "        # self.max_pool2 = nn.MaxPool2d(2)            # in = (64,128,128) | out = (64,64,64)\n",
    "        # self.channel_inc1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding='same') # in = (64,64,64) | out = (128,64,64)\n",
    "\n",
    "        self.resblocks128 = nn.Sequential(\n",
    "            *(3*[ResBlock128()]+[downsampleBlock(128,256)]))                     # in = (128,64,64) | out = (128,64,64)\n",
    "        # self.max_pool3 = nn.MaxPool2d(2)            # in = (128,64,64) | out = (128,32,32)\n",
    "        # self.channel_inc2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), padding='same')   # in = (128,32,32) | out = (256,32,32)\n",
    "\n",
    "        self.resblocks256 = nn.Sequential(      \n",
    "            *(5*[ResBlock256()]+[downsampleBlock(256,512)]))                    # in = (256,32,32) | out = (256,32,32)\n",
    "        # self.max_pool4 = nn.MaxPool2d(2)            # in = (256,16,16) | out = (256,16,16)\n",
    "        # self.channel_inc3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3,3), padding='same')   # in = (512,16,16) | out = (512,16,16)\n",
    "\n",
    "        self.resblocks512 = nn.Sequential(\n",
    "            *(2*[ResBlock512()]))                     # in = (512,16,16) | out = (512,16,16)\n",
    "        # self.max_pool5 = nn.MaxPool2d(2)            # in = (512,8,8) | out = (512,8,8)\n",
    "\n",
    "        # Now we would use a Global Average Pooling Layer which is just a mean on the first two image dimensions excluding the channel dim.\n",
    "        # But that would just be an operation in the forward pass\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bfc2 = nn.BatchNorm1d(128)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "\n",
    "        # self.fc3 = nn.Linear(512, 128)\n",
    "        # self.bfc3 = nn.BatchNorm2d(128)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Now we will need to define our output layers\n",
    "        # computation for the multi-label heads\n",
    "        \n",
    "        self.head1 = nn.Linear(128,2)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.head2 = nn.Linear(128,2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        self.head3 = nn.Linear(128,2)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        self.head4 = nn.Linear(128,2)\n",
    "        self.sig4 = nn.Sigmoid()\n",
    "        self.head5 = nn.Linear(128,2)\n",
    "        self.sig5 = nn.Sigmoid()\n",
    "        self.head6 = nn.Linear(128,2)\n",
    "        self.sig6 = nn.Sigmoid()\n",
    "        self.head7 = nn.Linear(128,2)\n",
    "        self.sig7 = nn.Sigmoid()\n",
    "        self.head8 = nn.Linear(128,2)\n",
    "        self.sig8 = nn.Sigmoid()\n",
    "        self.head9 = nn.Linear(128,2)\n",
    "        self.sig9 = nn.Sigmoid()\n",
    "        self.head10 = nn.Linear(128,2)\n",
    "        self.sig10 = nn.Sigmoid()\n",
    "        self.head11 = nn.Linear(128,2)\n",
    "        self.sig11 = nn.Sigmoid()\n",
    "        self.head12 = nn.Linear(128,2)\n",
    "        self.sig12 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" This function would define the computations of the layers to produce the output\"\"\"\n",
    "        # assert X.shape == (2,3,224,224)\n",
    "        # print(self.input_conv(X).shape)\n",
    "        # output of the first layer conv and maxpool\n",
    "        out = self.max_pool1(self.input_conv(X))\n",
    "\n",
    "        # output of the first resblock, maxpool2, channel_inc1\n",
    "        out = self.resblocks64(out)\n",
    "\n",
    "        # output of  resblock128\n",
    "        out = self.resblocks128(out)\n",
    "\n",
    "        # output after resblock256\n",
    "        out = self.resblocks256(out)\n",
    "\n",
    "        # output after resblock 512\n",
    "        out =  self.resblocks512(out)\n",
    "\n",
    "        # Then we would use a global average pooling layer i.e mean on the image_height and width using torch.mean()\n",
    "        out = torch.mean(out, dim = (2,3), keepdim=True)\n",
    "        # Now we will reshape the tensor to have a\n",
    "        # out = out.view(128,-1)\n",
    "\n",
    "        out = out.view(self.batch_size,-1)\n",
    "        # output of the first fully conected layer\n",
    "        # out = self.relu1(self.bfc1(self.fc1(out)))\n",
    "        # print(f\"The output after the average pooling op is {out.shape}\")\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.bfc2(out)\n",
    "        out = self.relu_fc2(out)\n",
    "        # output after second fully connected layer\n",
    "        # out = self.relu_fc2(self.bfc2(self.fc2(out)))\n",
    "\n",
    "        # output after the third fully connected layer\n",
    "        # out = self.relu3(self.bfc3(self.fc3(out)))\n",
    "\n",
    "\n",
    "        # Now we will define the computations of the 12 heads and heads and put all the outputs into one tensor and return that tensor.\n",
    "        self.out_head1 = self.sig1(self.head1(out))  # 'food'\n",
    "        self.out_head2 = self.sig2(self.head2(out)) # animal\n",
    "        self.out_head3 = self.sig3(self.head3(out)) # furniture\n",
    "        self.out_head4 = self.sig4(self.head4(out)) # electronic\n",
    "        self.out_head5 = self.sig5(self.head5(out)) # kitchen\n",
    "        self.out_head6 = self.sig6(self.head6(out)) # vehicle\n",
    "        self.out_head7 = self.sig7(self.head7(out)) # person\n",
    "        self.out_head8 = self.sig8(self.head8(out)) # outdoor\n",
    "        self.out_head9 = self.sig9(self.head9(out)) # accessory\n",
    "        self.out_head10 = self.sig10(self.head10(out)) # sports\n",
    "        self.out_head11 = self.sig11(self.head11(out)) # appliance\n",
    "        self.out_head12 = self.sig12(self.head12(out)) # indoor\n",
    "        out_list = [self.out_head1,self.out_head2,self.out_head3,self.out_head4,self.out_head5,self.out_head6,self.out_head7,self.out_head8,\n",
    "                    self.out_head9,self.out_head10,self.out_head11,self.out_head12]\n",
    "\n",
    "        out_tensor = torch.stack(out_list, dim = 1)\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,labels,_ = next(iter(test_coco_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_c = ResNet(batch_size = 16)\n",
    "# writer.add_graph(res_c, images)\n",
    "# writer.close()\n",
    "# summary(res_c, input_size = (16,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now lets get a batch of data and try to our model into our data.\n",
    "# exp_train_data, exp_train_label = [],[]\n",
    "\n",
    "# for i in range(32):\n",
    "#     img, labels,_ = train_coco[i]\n",
    "#     exp_train_data.append(img)\n",
    "#     exp_train_label.append(labels)\n",
    "# # exp_train_data = torch.tensor(exp_train_data)\n",
    "# # exp_train_label = torch.tensor(exp_train_label)\n",
    "# # print(exp_train_data.shape)\n",
    "# # print(exp_train_label.shape)\n",
    "# # print(len(exp_train_data), len(exp_train_label))\n",
    "# exp_train_data = torch.stack(exp_train_data)\n",
    "# exp_train_label = torch.stack(exp_train_label)\n",
    "# exp_train_data.shape\n",
    "\n",
    "# exp_train_data = exp_train_data.to(device=device)\n",
    "# exp_train_label = exp_train_label.to(device=device)\n",
    "\n",
    "# res_c = res_c.to(device=device)\n",
    "# res_c(exp_train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we  will try to train the resnet on one 1300 images of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No we will define the model\n",
    "# res_c = res_c.to(device = device)\n",
    "# bce_loss = torch.nn.BCELoss()\n",
    "# optimizer_adam = optim.Adam(res_c.parameters(), lr = 0.1)\n",
    "# scehdular = optim.lr_scheduler.StepLR(optimizer= optimizer_adam, gamma = 0.1, step_size = 20, verbose = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop(\n",
    "#     epochs = 100,\n",
    "#     conv_model=res_c,\n",
    "#     train_dataloader=test_coco_dataloader,\n",
    "#     device = device,\n",
    "#     optimizer=optimizer_adam,\n",
    "#     loss_fn=bce_loss,\n",
    "#     val_dataloader=val_dataloader,\n",
    "#     schedular = scehdular    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_50 = torchvision.models.resnet50(weights =ResNet50_Weights.DEFAULT)\n",
    "print(summary(res_50, input_size=(16,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would create out custom model with 12 heads and using the resnet50 model as the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, pretrained_model = None):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "        for parameter in pretrained_model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "\n",
    "        pretrained_model.fc = nn.Linear(2048,1024)\n",
    "        # pretrained_model.b0 = nn.BatchNorm1d(1024, track_running_stats=True)\n",
    "        self.backbone = nn.Sequential(pretrained_model)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024,512)\n",
    "        self.b1 = nn.BatchNorm1d(512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.b2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Now we will need to define our output layers\n",
    "        # computation for the multi-label heads\n",
    "        \n",
    "        self.head1 = nn.Linear(128,2)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.head2 = nn.Linear(128,2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        self.head3 = nn.Linear(128,2)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        self.head4 = nn.Linear(128,2)\n",
    "        self.sig4 = nn.Sigmoid()\n",
    "        self.head5 = nn.Linear(128,2)\n",
    "        self.sig5 = nn.Sigmoid()\n",
    "        self.head6 = nn.Linear(128,2)\n",
    "        self.sig6 = nn.Sigmoid()\n",
    "        self.head7 = nn.Linear(128,2)\n",
    "        self.sig7 = nn.Sigmoid()\n",
    "        self.head8 = nn.Linear(128,2)\n",
    "        self.sig8 = nn.Sigmoid()\n",
    "        self.head9 = nn.Linear(128,2)\n",
    "        self.sig9 = nn.Sigmoid()\n",
    "        self.head10 = nn.Linear(128,2)\n",
    "        self.sig10 = nn.Sigmoid()\n",
    "        self.head11 = nn.Linear(128,2)\n",
    "        self.sig11 = nn.Sigmoid()\n",
    "        self.head12 = nn.Linear(128,2)\n",
    "        self.sig12 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        out_backbone = self.backbone(X)\n",
    "\n",
    "        out  = self.relu1(self.b1(self.fc1(out_backbone)))\n",
    "        out = self.relu2(self.b2(self.fc2(out)))\n",
    "\n",
    "        \n",
    "        # Now we will define the computations of the 12 heads and heads and put all the outputs into one tensor and return that tensor.\n",
    "        self.out_head1 = self.sig1(self.head1(out))  # 'food'\n",
    "        self.out_head2 = self.sig2(self.head2(out)) # animal\n",
    "        self.out_head3 = self.sig3(self.head3(out)) # furniture\n",
    "        self.out_head4 = self.sig4(self.head4(out)) # electronic\n",
    "        self.out_head5 = self.sig5(self.head5(out)) # kitchen\n",
    "        self.out_head6 = self.sig6(self.head6(out)) # vehicle\n",
    "        self.out_head7 = self.sig7(self.head7(out)) # person\n",
    "        self.out_head8 = self.sig8(self.head8(out)) # outdoor\n",
    "        self.out_head9 = self.sig9(self.head9(out)) # accessory\n",
    "        self.out_head10 = self.sig10(self.head10(out)) # sports\n",
    "        self.out_head11 = self.sig11(self.head11(out)) # appliance\n",
    "        self.out_head12 = self.sig12(self.head12(out)) # indoor\n",
    "        out_list = [self.out_head1,self.out_head2,self.out_head3,self.out_head4,self.out_head5,self.out_head6,self.out_head7,self.out_head8,\n",
    "                    self.out_head9,self.out_head10,self.out_head11,self.out_head12]\n",
    "\n",
    "        out_tensor = torch.stack(out_list, dim = 1)\n",
    "        return out_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomResNet(pretrained_model=res_50)\n",
    "print(summary(custom_model, input_size=(128,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('../runs/resnet_exp_2_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = custom_model.to(device = device)\n",
    "bce_loss = torch.nn.BCELoss()\n",
    "optimizer_adam = optim.Adam(custom_model.parameters(), lr = 0.001)\n",
    "scehdular = optim.lr_scheduler.StepLR(optimizer= optimizer_adam, gamma = 0.01, step_size = 30, verbose = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "    epochs = 10,\n",
    "    conv_model=custom_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    device = device,\n",
    "    optimizer=optimizer_adam,\n",
    "    loss_fn=bce_loss,\n",
    "    val_dataloader=val_dataloader,\n",
    "    schedular = scehdular,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save the model\n",
    "torch.save(custom_model.state_dict(), 'F://coco/saved_models/custom_ResNet_pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0,66000, 6))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "dt.datetime.now()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam schedular(gamma = 0.1)\n",
    "        lr = 3e-5\n",
    "        weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5863a01bb4350d9241febf9e57f76b3c44dc4260331656e165259b66bc149002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
